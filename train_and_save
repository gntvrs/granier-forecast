from google.cloud import bigquery, storage
import pandas as pd
import numpy as np
import uuid, joblib
from datetime import datetime
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

PROJECT = "business-intelligence-444511"
DATASET = "granier_logistica"
TABLE   = "ConsumoEntrenamiento_Semanal_V2"
BUCKET  = "granier-modelos"

def load_df():
    client = bigquery.Client(project=PROJECT, location="europe-southwest1")
    q = f"SELECT * FROM `{PROJECT}.{DATASET}.{TABLE}`"
    df = client.query(q).to_dataframe()
    # Orden temporal por fecha ISO de la semana
    df["Semana_Date"] = pd.to_datetime(df["Semana_Martes"].apply(lambda s: datetime.strptime(s + "-1", "%G-W%V-%u").date()))
    df = df.sort_values(["Semana_Date","Centro","Articulo"]).reset_index(drop=True)
    return df

def build_features(df: pd.DataFrame):
    target = "Volumen_Semana"
    cat = ["Centro","Grupo_articulo"]                # puedes añadir Articulo como categórica si procede
    num = [
        "Lag_1","Lag_2","Lag_3","Lag_4","Lag_5","Lag_6","Lag_7","Lag_8",
        "MA_3","MA_4","MA_6","MA_8","STD_8","Vol_Ym1","Crecimiento_WoW_Ym1"
    ]
    # Filtra filas con NaNs en lags iniciales (o imputa)
    X = df[cat + num].copy()
    y = df[target].astype(float).values

    pre = ColumnTransformer([
        ("cat", Pipeline([("imp", SimpleImputer(strategy="most_frequent")),
                          ("ohe", OneHotEncoder(handle_unknown="ignore"))]), cat),
        ("num", Pipeline([("imp", SimpleImputer(strategy="median"))]), num)
    ])
    return X, y, pre

def time_series_cv_splits(df, n_splits=5):
    # CV por semana, manteniendo orden temporal
    # Usa índices del dataframe ya ordenado
    tscv = TimeSeriesSplit(n_splits=n_splits)
    return tscv

def evaluate(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = mean_squared_error(y_true, y_pred, squared=False)
    r2 = r2_score(y_true, y_pred)
    return {"MAE": mae, "RMSE": rmse, "R2": r2}

def train_and_save():
    df = load_df()
    X, y, pre = build_features(df)
    splits = time_series_cv_splits(df, n_splits=5)

    # ========= Modelo 1: RandomForest (punto) =========
    rf = Pipeline(steps=[
        ("pre", pre),
        ("rf", RandomForestRegressor(random_state=42, n_estimators=400, max_depth=None, n_jobs=-1))
    ])
    rf_param = {
        "rf__n_estimators": [400],
        "rf__max_depth": [None, 12, 18],
        "rf__min_samples_leaf": [1, 3]
    }
    rf_cv = GridSearchCV(rf, rf_param, cv=splits, scoring="neg_mean_absolute_error", n_jobs=-1)
    rf_cv.fit(X, y)
    rf_best = rf_cv.best_estimator_

    y_pred_rf = rf_best.predict(X)
    metrics_rf = evaluate(y, y_pred_rf)

    # ========= Modelo 2: GradientBoosting (cuantil 0.75) =========
    # Sesga la predicción hacia el percentil superior para reducir riesgo de rotura
    gbr_q = Pipeline(steps=[
        ("pre", pre),
        ("gbr", GradientBoostingRegressor(loss="quantile", alpha=0.75, random_state=42))
    ])
    gbr_param = {
        "gbr__n_estimators": [300, 500],
        "gbr__max_depth": [2, 3],
        "gbr__learning_rate": [0.05, 0.1],
        "gbr__min_samples_leaf": [1, 3]
    }
    gbr_cv = GridSearchCV(gbr_q, gbr_param, cv=splits, scoring="neg_mean_absolute_error", n_jobs=-1)
    gbr_cv.fit(X, y)
    gbr_best = gbr_cv.best_estimator_

    y_pred_gbr = gbr_best.predict(X)
    metrics_gbr = evaluate(y, y_pred_gbr)

    print("RF (punto)  ->", metrics_rf)
    print("GBR (q=0.75)->", metrics_gbr)

    # ========= Guardado a GCS =========
    ts = pd.Timestamp.today().strftime('%Y%m%d')
    uid = uuid.uuid4().hex[:6]

    storage_client = storage.Client(project=PROJECT)
    bucket = storage_client.bucket(BUCKET)

    # RF
    rf_name = f"consumo/modelo_V2_RF_{ts}_{uid}.pkl"
    joblib.dump(rf_best, "/tmp/rf.pkl")
    bucket.blob(rf_name).upload_from_filename("/tmp/rf.pkl")

    # GBR cuantil
    gbr_name = f"consumo/modelo_V2_GBRq75_{ts}_{uid}.pkl"
    joblib.dump(gbr_best, "/tmp/gbr.pkl")
    bucket.blob(gbr_name).upload_from_filename("/tmp/gbr.pkl")

    # Actualiza punteros “latest”
    bucket.blob("consumo/last_model_v2_rf.txt").upload_from_string(rf_name.split("/")[-1])
    bucket.blob("consumo/last_model_v2_gbrq75.txt").upload_from_string(gbr_name.split("/")[-1])

if __name__ == "__main__":
    train_and_save()
