from google.cloud import bigquery, storage
import pandas as pd
import numpy as np
import uuid, joblib
from datetime import datetime
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

PROJECT = "business-intelligence-444511"
DATASET = "granier_logistica"
TABLE   = "ConsumoEntrenamiento_Semanal_V2"   # V2 con YoY limpio + shift correcto
BUCKET  = "granier-modelos"

def load_df_fast():
    client = bigquery.Client(project=PROJECT, location="europe-southwest1")
    q = f"""
    SELECT
      Semana_Martes, Articulo, Centro, Grupo_articulo,
      Volumen_Semana,
      Lag_1,Lag_2,Lag_3,Lag_4,Lag_5,Lag_6,Lag_7,Lag_8,
      MA_3,MA_4,MA_6,MA_8,STD_8,
      Vol_Ym1_clean, Crec_clean, YoY_level, Delta_MA3_vs_YoY
    FROM `{PROJECT}.{DATASET}.{TABLE}`
    """
    df = client.query(q).to_dataframe(create_bqstorage_client=True)
    # Semana ISO → lunes (fecha de orden)
    df["Semana_Date"] = pd.to_datetime(df["Semana_Martes"].apply(
        lambda s: datetime.strptime(s + "-1", "%G-W%V-%u").date()
    ))
    df = df.sort_values(["Semana_Date","Centro","Articulo"]).reset_index(drop=True)
    return df

def build_features(df: pd.DataFrame):
    target = "Volumen_Semana"
    cat = ["Centro","Grupo_articulo"]
    num = [
        # señal temporal
        "Lag_1","Lag_2","Lag_3","Lag_4","Lag_5","Lag_6","Lag_7","Lag_8",
        "MA_3","MA_4","MA_6","MA_8","STD_8",
        # señal YoY saneada y derivada
        "Vol_Ym1_clean","Crec_clean","YoY_level","Delta_MA3_vs_YoY",
    ]

    # Exige un mínimo de historia
    df = df.dropna(subset=["Lag_1","Lag_2","Lag_3"]).copy()

    # (debug) columnas numéricas completamente vacías → error explícito
    empty_full = [c for c in num if c in df.columns and not df[c].notna().any()]
    if empty_full:
        raise ValueError(f"Columnas 100% vacías en el dataset: {empty_full}. Revisa la V2 o el rango temporal.")

    X = df[cat + num]
    y = df[target].astype(float).values

    # Seguridad: no deben quedar NaNs después del filtro anterior
    if X.isnull().sum().sum() > 0:
        # Imputer se encargará de NaNs parciales, pero avisamos si hay demasiados
        print("[WARN] Quedan NaNs en X tras el filtro; el Imputer los cubrirá.")

    pre = ColumnTransformer([
        ("cat", Pipeline([
            ("imp", SimpleImputer(strategy="most_frequent")),
            ("ohe", OneHotEncoder(handle_unknown="ignore"))
        ]), cat),
        ("num", SimpleImputer(strategy="median"), num),
    ])
    return df, X, y, pre

def last_weeks_split(df, weeks_holdout=8):
    last_date = df["Semana_Date"].max()
    cutoff = last_date - pd.Timedelta(days=7*weeks_holdout)
    train_idx = df["Semana_Date"] <= cutoff
    test_idx  = df["Semana_Date"] >  cutoff
    return train_idx.values, test_idx.values

def evaluate(y_true, y_pred):
    return {
        "MAE":  float(mean_absolute_error(y_true, y_pred)),
        "RMSE": float(mean_squared_error(y_true, y_pred, squared=False)),
        "R2":   float(r2_score(y_true, y_pred)),
        "Bias": float(np.mean(y_pred - y_true))
    }

def entrenar_y_guardar_modelo():
    df = load_df_fast()
    print(f"[INFO] filas totales: {len(df):,}")
    df, X, y, pre = build_features(df)
    tr, te = last_weeks_split(df, weeks_holdout=8)
    print(f"[INFO] train: {tr.sum():,}  |  valid: {te.sum():,}")

    # ===== RF =====
    rf = Pipeline(steps=[
        ("pre", pre),
        ("rf", RandomForestRegressor(
            n_estimators=200, max_depth=None, min_samples_leaf=2,
            n_jobs=-1, random_state=42
        ))
    ])
    rf.fit(X.iloc[tr], y[tr])
    m_rf_tr = evaluate(y[tr], rf.predict(X.iloc[tr]))
    m_rf_te = evaluate(y[te], rf.predict(X.iloc[te]))

    # ===== GBR cuantil 0.75 =====
    gbr = Pipeline(steps=[
        ("pre", pre),
        ("gbr", GradientBoostingRegressor(
            loss="quantile", alpha=0.75,
            n_estimators=200, max_depth=2, learning_rate=0.1,
            random_state=42
        ))
    ])
    gbr.fit(X.iloc[tr], y[tr])
    m_gbr_tr = evaluate(y[tr], gbr.predict(X.iloc[tr]))
    m_gbr_te = evaluate(y[te], gbr.predict(X.iloc[te]))

    print("RF  (train):", m_rf_tr)
    print("RF  (valid):", m_rf_te)
    print("Q75 (train):", m_gbr_tr)
    print("Q75 (valid):", m_gbr_te)

    # ===== Guardado en GCS =====
    ts  = pd.Timestamp.today().strftime('%Y%m%d')
    uid = uuid.uuid4().hex[:6]
    rf_name  = f"consumo/modelo_V2_FAST_RF_{ts}_{uid}.pkl"
    q75_name = f"consumo/modelo_V2_FAST_Q75_{ts}_{uid}.pkl"

    joblib.dump(rf,  "/tmp/rf.pkl")
    joblib.dump(gbr, "/tmp/q75.pkl")

    storage_client = storage.Client(project=PROJECT)
    bucket = storage_client.bucket(BUCKET)
    bucket.blob(rf_name).upload_from_filename("/tmp/rf.pkl")
    bucket.blob(q75_name).upload_from_filename("/tmp/q75.pkl")

    bucket.blob("consumo/last_model_v2_fast_rf.txt").upload_from_string(rf_name.split("/")[-1])
    bucket.blob("consumo/last_model_v2_fast_q75.txt").upload_from_string(q75_name.split("/")[-1])

if __name__ == "__main__":
    entrenar_y_guardar_modelo()

